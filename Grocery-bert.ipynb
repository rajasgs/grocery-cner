{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0ba79d-259a-4dbf-9c34-ebf34b00fb04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install seqeval\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3d17e1-0c96-4315-9240-5f564354342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n"
     ]
    }
   ],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821c6695-23b0-4dd1-9346-31150aadc370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05f3465-1579-4129-bb22-045c21b00465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0126cd9d-726d-4ddb-a1fc-6f0aa3044375",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['0', 'FOOD_ITEM', 'MEASUREMENT']  # Update with your label classes\n",
    "label_encoding_dict = {'0': 0, 'FOOD_ITEM': 1, 'MEASUREMENT': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a13cd51-0364-41aa-a7eb-9e4718e1567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f357fad3-e5e4-4cd0-8554-6f7ae45479da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5238bd3-525b-4d87-886c-70a77007e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Tag'] = df['Tag'].replace(label_encoding_dict)  # Encode 'Tag' column according to label_encoding_dict\n",
    "    split_list = [list(y) for x, y in itertools.groupby(df.values.tolist(), lambda z: pd.isna(z[0])) if not x]\n",
    "    tokens = [[x[1] for x in y] for y in split_list]\n",
    "    entities = [[x[2] for x in y] for y in split_list]\n",
    "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9a8bb4-b98d-40ed-b0b8-c062d6171d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_dataset('~/datasets/grocery_training_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "801ea3cf-7980-4923-a391-8dbd392c4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_dataset('~/datasets/grocery_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0523895d-842d-45b1-96ca-b9181c63603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I, need, 2KG, of, samba, rice, and, 2, ltr, c...</td>\n",
       "      <td>[0, 0, 2, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [I, need, 2KG, of, samba, rice, and, 2, ltr, c...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 2, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7ffb17-487d-4bce-bdca-5383795ec9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e506fae-ba8a-451c-93b9-d889b38adebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e40a97b2-b872-4eac-92be-e67c367fb2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30d22e87-b9e6-49d3-9710-9906fb984ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, padding=\"max_length\", is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == 0:\n",
    "                label_ids.append(0)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01da72cd-522c-49ee-9aa9-68c29a0b971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 96.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5041e6f-cf35-4980-a43d-98d6ddf5b3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 259.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20cdf170-d9dc-4be5-aa58-6745fbc8717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc3e135f-d717-4547-a5db-d5b5b906390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff34a252-7bc8-4931-a000-539f16123378",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e39e88d3-0d80-4564-b978-5b328b0812eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1995200/4079666221.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61b078c9-bb7e-4890-afcf-2a474f54224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4af4be2-35fc-455b-9d87-59112dbcd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "     eval_dataset=test_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb264d45-3abe-4d52-84da-54730e02fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597177</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.473856</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425844</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: FOOD_ITEM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MEASUREMENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/rajaraman/miniconda3/envs/dl39/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6779602368672689, metrics={'train_runtime': 5.6408, 'train_samples_per_second': 0.532, 'train_steps_per_second': 0.532, 'total_flos': 783897357312.0, 'train_loss': 0.6779602368672689, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d43417a-6f60-44dc-9175-6b0fa0d5ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeae1846-c38d-48ab-af01-01ca11709452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ner_model = pipeline(\"ner\", model=trainer.model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83f776f2-7e15-4600-b0a5-cd87e60db349",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"I need 2KG of samba rice and 2 ltr coconut oil.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a96bf57-92fd-435a-a3f5-0d21592ac695",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_label_encoding_dict = {v: k for k, v in label_encoding_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22881518-df51-47b7-ba20-1059c0bc04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = ner_model(input_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aeb02d7-621e-4aec-93a9-41ce11527508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: i\tLabel: 0\n",
      "Token: need\tLabel: 0\n",
      "Token: 2\tLabel: 0\n",
      "Token: ##k\tLabel: FOOD_ITEM\n",
      "Token: ##g\tLabel: 0\n",
      "Token: of\tLabel: 0\n",
      "Token: samba\tLabel: FOOD_ITEM\n",
      "Token: rice\tLabel: FOOD_ITEM\n",
      "Token: and\tLabel: 0\n",
      "Token: 2\tLabel: 0\n",
      "Token: lt\tLabel: 0\n",
      "Token: ##r\tLabel: 0\n",
      "Token: coconut\tLabel: FOOD_ITEM\n",
      "Token: oil\tLabel: FOOD_ITEM\n",
      "Token: .\tLabel: FOOD_ITEM\n"
     ]
    }
   ],
   "source": [
    "for result in ner_results:\n",
    "    token = result[\"word\"]\n",
    "    label = result[\"entity\"]\n",
    "    if label == \"LABEL_0\":\n",
    "      label= \"0\"\n",
    "    elif label == \"LABEL_1\":\n",
    "      label = \"FOOD_ITEM\"\n",
    "    else:\n",
    "      label = \"MEASUREMENT\"\n",
    "    print(f\"Token: {token}\\tLabel: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa2064-e965-47a8-9906-a0602b05a3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22744527-6e50-4eb3-b27f-c554167bd4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
